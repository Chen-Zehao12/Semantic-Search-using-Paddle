{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T15:13:18.622465Z",
     "iopub.status.busy": "2022-08-05T15:13:18.621751Z",
     "iopub.status.idle": "2022-08-05T15:13:18.628471Z",
     "shell.execute_reply": "2022-08-05T15:13:18.627543Z",
     "shell.execute_reply.started": "2022-08-05T15:13:18.622413Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/work\n"
     ]
    }
   ],
   "source": [
    "%cd /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T15:13:18.632795Z",
     "iopub.status.busy": "2022-08-05T15:13:18.632268Z",
     "iopub.status.idle": "2022-08-05T15:13:21.122508Z",
     "shell.execute_reply": "2022-08-05T15:13:21.121544Z",
     "shell.execute_reply.started": "2022-08-05T15:13:18.632757Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: paddlenlp==2.3.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.3.4)\n",
      "Requirement already satisfied: paddlefsl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (1.1.0)\n",
      "Requirement already satisfied: dill<0.3.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (0.3.3)\n",
      "Requirement already satisfied: multiprocess<=0.70.12.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (0.70.11.1)\n",
      "Requirement already satisfied: paddle2onnx in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (0.9.8)\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (0.42.1)\n",
      "Requirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (1.2.2)\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (0.4.4)\n",
      "Requirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (4.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (4.64.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (0.1.96)\n",
      "Requirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (3.20.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.4) (2.4.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (2022.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (0.8.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (21.3)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (3.8.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (3.0.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (4.2.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (9.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (0.18.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (1.1.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (1.19.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.4) (2.24.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp==2.3.4) (0.24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp==2.3.4) (4.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp==2.3.4) (3.0.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp==2.3.4) (5.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging->datasets>=2.0.0->paddlenlp==2.3.4) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.4) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.4) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.4) (2.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.4) (3.0.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.3.4) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.3.4) (1.6.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.3.4) (0.14.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.4) (1.8.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.4) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.4) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.4) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.4) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.4) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.4) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.4) (2.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata->datasets>=2.0.0->paddlenlp==2.3.4) (3.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->datasets>=2.0.0->paddlenlp==2.3.4) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->datasets>=2.0.0->paddlenlp==2.3.4) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=2.0.0->paddlenlp==2.3.4) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp==2.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T15:13:21.124036Z",
     "iopub.status.busy": "2022-08-05T15:13:21.123741Z",
     "iopub.status.idle": "2022-08-05T15:13:24.106275Z",
     "shell.execute_reply": "2022-08-05T15:13:24.105384Z",
     "shell.execute_reply.started": "2022-08-05T15:13:21.124010Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入系统库\n",
    "import abc\n",
    "import sys\n",
    "from functools import partial\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "# 导入python的其他库\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from scipy.special import softmax\n",
    "from scipy.special import expit\n",
    "# 导入Paddle库\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle import inference\n",
    "\n",
    "#导入PaddleNLP相关的库\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.datasets import load_dataset, MapDataset\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "from paddlenlp.utils.downloader import get_path_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T15:13:24.108239Z",
     "iopub.status.busy": "2022-08-05T15:13:24.107519Z",
     "iopub.status.idle": "2022-08-05T15:13:24.112549Z",
     "shell.execute_reply": "2022-08-05T15:13:24.111916Z",
     "shell.execute_reply.started": "2022-08-05T15:13:24.108206Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 关键参数\n",
    "scale=20 # 推荐值: 10 ~ 30\n",
    "margin=0.1 # 推荐值: 0.0 ~ 0.2\n",
    "# SimCSE的dropout的参数，也可以使用预训练语言模型默认的dropout参数\n",
    "dropout=0.2\n",
    "# 向量映射的维度，默认的输出是768维，推荐通过线性层映射成256维\n",
    "output_emb_size=256\n",
    "# 语义索引的维度，可以根据自己的情况调节长度\n",
    "max_seq_length=140\n",
    "# 根据经验 batch_size越大效果越好\n",
    "batch_size=30\n",
    "# 训练的epoch数目\n",
    "epochs=1\n",
    "weight_decay=0.0\n",
    "# 学习率\n",
    "learning_rate=5E-5\n",
    "warmup_proportion=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T15:13:24.113791Z",
     "iopub.status.busy": "2022-08-05T15:13:24.113486Z",
     "iopub.status.idle": "2022-08-05T15:13:33.025744Z",
     "shell.execute_reply": "2022-08-05T15:13:33.024900Z",
     "shell.execute_reply.started": "2022-08-05T15:13:24.113767Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-05 23:13:24,116] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-m-large/ernie_m_large.pdparams\n",
      "W0805 23:13:24.120151  5719 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0805 23:13:24.124575  5719 gpu_resources.cc:91] device: 0, cuDNN Version: 7.6.\n",
      "[2022-08-05 23:13:32,122] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-m-large/ernie_m.vocab.txt\n",
      "[2022-08-05 23:13:32,126] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-m-large/ernie_m.sentencepiece.bpe.model\n",
      "[2022-08-05 23:13:32,991] [    INFO] - tokenizer config file saved in /home/aistudio/.paddlenlp/models/ernie-m-large/tokenizer_config.json\n",
      "[2022-08-05 23:13:32,994] [    INFO] - Special tokens file saved in /home/aistudio/.paddlenlp/models/ernie-m-large/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# 使用erine-m模型\n",
    "MODEL_NAME_OR_PATH = \"ernie-m-large\"\n",
    "# 从保存的参数中读取\n",
    "# MODEL_NAME_OR_PATH = 'checkpoint'\n",
    "pretrained_model = ppnlp.transformers.ErnieMModel.from_pretrained(\n",
    "    MODEL_NAME_OR_PATH, \n",
    "    hidden_dropout_prob=dropout,\n",
    "    attention_probs_dropout_prob=dropout)\n",
    "# 定义模型对应的tokenizer，tokenizer可以把原始输入文本转化成模型model可接受的输入数据格式。需注意tokenizer类要与选择的模型相对应，具体可以查看PaddleNLP相关文档\n",
    "tokenizer = ppnlp.transformers.ErnieMTokenizer.from_pretrained(MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T15:13:33.027503Z",
     "iopub.status.busy": "2022-08-05T15:13:33.027078Z",
     "iopub.status.idle": "2022-08-05T15:13:38.272290Z",
     "shell.execute_reply": "2022-08-05T15:13:38.271316Z",
     "shell.execute_reply.started": "2022-08-05T15:13:33.027471Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 数据读取逻辑\n",
    "def read_simcse_text(data_path):\n",
    "    \"\"\"Reads data.\"\"\"\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if(i==0):\n",
    "                continue\n",
    "            data = line.rstrip().split('\\t')\n",
    "            if len(data) != 4:\n",
    "                continue\n",
    "            data = data[2]\n",
    "            # 这里的text_a和text_b是一样的\n",
    "            yield {'text_a': data, 'text_b': data}\n",
    "\n",
    "train_set_file='data/multilingual/data.csv'\n",
    "train_ds = load_dataset(read_simcse_text, data_path=train_set_file, lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T15:13:38.274104Z",
     "iopub.status.busy": "2022-08-05T15:13:38.273689Z",
     "iopub.status.idle": "2022-08-05T15:13:38.285367Z",
     "shell.execute_reply": "2022-08-05T15:13:38.284314Z",
     "shell.execute_reply.started": "2022-08-05T15:13:38.274074Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 明文数据 -> ID 序列训练数据\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      batchify_fn=None,\n",
    "                      trans_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == 'train':\n",
    "        # 分布式批采样器加载数据的一个子集。\n",
    "        # 每个进程可以传递给DataLoader一个DistributedBatchSampler的实例，每个进程加载原始数据的一个子集。\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    else:\n",
    "        # 批采样器的基础实现，\n",
    "        # 用于 paddle.io.DataLoader 中迭代式获取mini-batch的样本下标数组，数组长度与 batch_size 一致。\n",
    "        batch_sampler = paddle.io.BatchSampler(\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    # 组装mini-batch\n",
    "    return paddle.io.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "def convert_example(example, tokenizer, max_seq_length=512, do_evalute=False):\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for key, text in example.items():\n",
    "        if 'label' in key:\n",
    "            # do_evaluate\n",
    "            result += [example['label']]\n",
    "        else:\n",
    "            # do_train\n",
    "            encoded_inputs = tokenizer(text=text, max_seq_len=max_seq_length)\n",
    "            input_ids = encoded_inputs[\"input_ids\"]\n",
    "            result += [input_ids]\n",
    "\n",
    "    return result\n",
    "\n",
    "# 给convert_example赋予默认的值，如tokenizer，max_seq_length\n",
    "trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=max_seq_length)\n",
    "# [pad]对齐的函数\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # query_input\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # title_input\n",
    "    ): [data for data in fn(samples)]\n",
    "\n",
    "# 构建训练的Dataloader\n",
    "train_data_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        mode='train',\n",
    "        batch_size=batch_size,\n",
    "        batchify_fn=batchify_fn,\n",
    "        trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T15:13:38.287137Z",
     "iopub.status.busy": "2022-08-05T15:13:38.286724Z",
     "iopub.status.idle": "2022-08-05T15:13:38.300493Z",
     "shell.execute_reply": "2022-08-05T15:13:38.299485Z",
     "shell.execute_reply.started": "2022-08-05T15:13:38.287103Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SimCSE(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 pretrained_model,\n",
    "                 dropout=None,\n",
    "                 margin=0.0,\n",
    "                 scale=20,\n",
    "                 output_emb_size=None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.ptm = pretrained_model\n",
    "        # 显式的加一个dropout来控制\n",
    "        self.dropout = nn.Dropout(dropout if dropout is not None else 0.1)\n",
    "\n",
    "        # if output_emb_size is greater than 0, then add Linear layer to reduce embedding_size, \n",
    "        # 考虑到性能和效率，我们推荐把output_emb_size设置成256\n",
    "        # 向量越大，语义信息越丰富，但消耗资源越多\n",
    "        self.output_emb_size = output_emb_size\n",
    "        if output_emb_size > 0:\n",
    "            weight_attr = paddle.ParamAttr(\n",
    "                initializer=nn.initializer.TruncatedNormal(std=0.02))\n",
    "            self.emb_reduce_linear = paddle.nn.Linear(\n",
    "                1024, output_emb_size, weight_attr=weight_attr)\n",
    "\n",
    "        self.margin = margin\n",
    "        # 为了使余弦相似度更容易收敛，我们选择把计算出来的余弦相似度扩大scale倍，一般设置成20左右\n",
    "        self.sacle = scale\n",
    "\n",
    "    # 加入jit注释能够把该提取向量的函数导出成静态图\n",
    "    # 对应input_id\n",
    "    @paddle.jit.to_static(input_spec=[paddle.static.InputSpec(shape=[None, None], dtype='int64')])\n",
    "    def get_pooled_embedding(self,\n",
    "                             input_ids,\n",
    "                             position_ids=None,\n",
    "                             attention_mask=None,\n",
    "                             with_pooler=True):\n",
    "\n",
    "        # Note: cls_embedding is poolerd embedding with act tanh \n",
    "        sequence_output, cls_embedding = self.ptm(input_ids, position_ids, attention_mask)\n",
    "\n",
    "        if with_pooler == False:\n",
    "            cls_embedding = sequence_output[:, 0, :]\n",
    "\n",
    "        if self.output_emb_size > 0:\n",
    "            cls_embedding = self.emb_reduce_linear(cls_embedding)\n",
    "        cls_embedding = self.dropout(cls_embedding)\n",
    "        # https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/functional/normalize_cn.html\n",
    "        cls_embedding = F.normalize(cls_embedding, p=2, axis=-1)\n",
    "        return cls_embedding\n",
    "\n",
    "    def forward(self,\n",
    "                query_input_ids,\n",
    "                title_input_ids,\n",
    "                query_position_ids=None,\n",
    "                query_attention_mask=None,\n",
    "                title_position_ids=None,\n",
    "                title_attention_mask=None):\n",
    "        \n",
    "        # 第 1 次编码: 文本经过无监督语义索引模型编码后的语义向量 \n",
    "        # [N, 768]\n",
    "        query_cls_embedding = self.get_pooled_embedding(\n",
    "            query_input_ids, query_position_ids, query_attention_mask)\n",
    "\n",
    "        # 第 2 次编码: 文本经过无监督语义索引模型编码后的语义向量 \n",
    "        # [N, 768]\n",
    "        title_cls_embedding = self.get_pooled_embedding(\n",
    "            title_input_ids, title_position_ids, title_attention_mask)\n",
    "\n",
    "        # 相似度矩阵: [N, N]\n",
    "        cosine_sim = paddle.matmul(\n",
    "            query_cls_embedding, title_cls_embedding, transpose_y=True)\n",
    "\n",
    "        # substract margin from all positive samples cosine_sim()\n",
    "        # 填充self.margin值，比如margin为0.2，query_cls_embedding.shape[0]=2 \n",
    "        # margin_diag: [0.2,0.2]\n",
    "        margin_diag = paddle.full(\n",
    "            shape=[query_cls_embedding.shape[0]],\n",
    "            fill_value=self.margin,\n",
    "            dtype=paddle.get_default_dtype())\n",
    "        # input paddle.diag(margin_diag): [[0.2,0],[0,0.2]]\n",
    "        # input cosine_sim : [[1.0,0.6],[0.6,1.0]]\n",
    "        # output cosine_sim: [[0.8,0.6],[0.6,0.8]]\n",
    "        cosine_sim = cosine_sim - paddle.diag(margin_diag)\n",
    "\n",
    "        # scale cosine to ease training converge\n",
    "        cosine_sim *= self.sacle\n",
    "\n",
    "        # 转化成多分类任务: 对角线元素是正例，其余元素为负例\n",
    "        # labels : [0,1,2,3]\n",
    "        labels = paddle.arange(0, query_cls_embedding.shape[0], dtype='int64')\n",
    "        # labels : [[0],[1],[2],[3]]\n",
    "        labels = paddle.reshape(labels, shape=[-1, 1])\n",
    "\n",
    "        # 交叉熵损失函数\n",
    "        loss = F.cross_entropy(input=cosine_sim, label=labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T15:13:38.302141Z",
     "iopub.status.busy": "2022-08-05T15:13:38.301573Z",
     "iopub.status.idle": "2022-08-05T15:13:38.311475Z",
     "shell.execute_reply": "2022-08-05T15:13:38.310819Z",
     "shell.execute_reply.started": "2022-08-05T15:13:38.302117Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 实例化SimCSE\n",
    "model = SimCSE(\n",
    "        pretrained_model,\n",
    "        margin=margin,\n",
    "        scale=scale,\n",
    "        output_emb_size=output_emb_size)\n",
    "# 训练的总步数\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "# warmpup操作，学习率先上升后下降\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps,\n",
    "                                         warmup_proportion)\n",
    "\n",
    "# Generate parameter names needed to perform weight decay.\n",
    "# All bias and LayerNorm parameters are excluded.\n",
    "decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "# 设置优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=weight_decay,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T15:13:38.314428Z",
     "iopub.status.busy": "2022-08-05T15:13:38.314094Z",
     "iopub.status.idle": "2022-08-05T17:12:07.849183Z",
     "shell.execute_reply": "2022-08-05T17:12:07.848286Z",
     "shell.execute_reply.started": "2022-08-05T15:13:38.314404Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 1.42684\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.43264\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.18040\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.14533\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.09658\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.08119\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.05392\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.12706\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.07865\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.05027\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.05499\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.01691\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.01912\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.03017\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.01588\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.02352\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.03384\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.00992\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.01051\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.02774\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.00607\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.02400\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.00789\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.00823\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.00504\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.00901\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.00364\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.00504\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.00677\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.00719\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.00647\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.00325\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.00619\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.03743\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.00659\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.00731\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.05100\n",
      "global step 380, epoch: 1, batch: 380, loss: 0.00353\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.00294\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.00970\n",
      "global step 410, epoch: 1, batch: 410, loss: 0.00392\n",
      "global step 420, epoch: 1, batch: 420, loss: 0.00659\n",
      "global step 430, epoch: 1, batch: 430, loss: 0.00788\n",
      "global step 440, epoch: 1, batch: 440, loss: 0.00254\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.00373\n",
      "global step 460, epoch: 1, batch: 460, loss: 0.00309\n",
      "global step 470, epoch: 1, batch: 470, loss: 0.00541\n",
      "global step 480, epoch: 1, batch: 480, loss: 0.00243\n",
      "global step 490, epoch: 1, batch: 490, loss: 0.01356\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.00737\n",
      "global step 510, epoch: 1, batch: 510, loss: 0.01153\n",
      "global step 520, epoch: 1, batch: 520, loss: 0.00875\n",
      "global step 530, epoch: 1, batch: 530, loss: 0.00442\n",
      "global step 540, epoch: 1, batch: 540, loss: 0.00855\n",
      "global step 550, epoch: 1, batch: 550, loss: 0.00303\n",
      "global step 560, epoch: 1, batch: 560, loss: 0.01194\n",
      "global step 570, epoch: 1, batch: 570, loss: 0.02003\n",
      "global step 580, epoch: 1, batch: 580, loss: 0.00277\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.00507\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.00280\n",
      "global step 610, epoch: 1, batch: 610, loss: 0.00172\n",
      "global step 620, epoch: 1, batch: 620, loss: 0.00312\n",
      "global step 630, epoch: 1, batch: 630, loss: 0.00213\n",
      "global step 640, epoch: 1, batch: 640, loss: 0.00212\n",
      "global step 650, epoch: 1, batch: 650, loss: 0.00245\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.00213\n",
      "global step 670, epoch: 1, batch: 670, loss: 0.00285\n",
      "global step 680, epoch: 1, batch: 680, loss: 0.00319\n",
      "global step 690, epoch: 1, batch: 690, loss: 0.00545\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.00931\n",
      "global step 710, epoch: 1, batch: 710, loss: 0.00302\n",
      "global step 720, epoch: 1, batch: 720, loss: 0.00438\n",
      "global step 730, epoch: 1, batch: 730, loss: 0.00284\n",
      "global step 740, epoch: 1, batch: 740, loss: 0.00379\n",
      "global step 750, epoch: 1, batch: 750, loss: 0.00817\n",
      "global step 760, epoch: 1, batch: 760, loss: 0.00239\n",
      "global step 770, epoch: 1, batch: 770, loss: 0.00306\n",
      "global step 780, epoch: 1, batch: 780, loss: 0.00281\n",
      "global step 790, epoch: 1, batch: 790, loss: 0.00328\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.00403\n",
      "global step 810, epoch: 1, batch: 810, loss: 0.00646\n",
      "global step 820, epoch: 1, batch: 820, loss: 0.00214\n",
      "global step 830, epoch: 1, batch: 830, loss: 0.00294\n",
      "global step 840, epoch: 1, batch: 840, loss: 0.02201\n",
      "global step 850, epoch: 1, batch: 850, loss: 0.00358\n",
      "global step 860, epoch: 1, batch: 860, loss: 0.00369\n",
      "global step 870, epoch: 1, batch: 870, loss: 0.00227\n",
      "global step 880, epoch: 1, batch: 880, loss: 0.00543\n",
      "global step 890, epoch: 1, batch: 890, loss: 0.00319\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.00323\n",
      "global step 910, epoch: 1, batch: 910, loss: 0.00203\n",
      "global step 920, epoch: 1, batch: 920, loss: 0.00318\n",
      "global step 930, epoch: 1, batch: 930, loss: 0.00254\n",
      "global step 940, epoch: 1, batch: 940, loss: 0.00259\n",
      "global step 950, epoch: 1, batch: 950, loss: 0.00135\n",
      "global step 960, epoch: 1, batch: 960, loss: 0.00522\n",
      "global step 970, epoch: 1, batch: 970, loss: 0.00150\n",
      "global step 980, epoch: 1, batch: 980, loss: 0.00410\n",
      "global step 990, epoch: 1, batch: 990, loss: 0.00204\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.00143\n",
      "global step 1010, epoch: 1, batch: 1010, loss: 0.00104\n",
      "global step 1020, epoch: 1, batch: 1020, loss: 0.00186\n",
      "global step 1030, epoch: 1, batch: 1030, loss: 0.00299\n",
      "global step 1040, epoch: 1, batch: 1040, loss: 0.00180\n",
      "global step 1050, epoch: 1, batch: 1050, loss: 0.00200\n",
      "global step 1060, epoch: 1, batch: 1060, loss: 0.00156\n",
      "global step 1070, epoch: 1, batch: 1070, loss: 0.00320\n",
      "global step 1080, epoch: 1, batch: 1080, loss: 0.00200\n",
      "global step 1090, epoch: 1, batch: 1090, loss: 0.00194\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.00353\n",
      "global step 1110, epoch: 1, batch: 1110, loss: 0.00293\n",
      "global step 1120, epoch: 1, batch: 1120, loss: 0.00142\n",
      "global step 1130, epoch: 1, batch: 1130, loss: 0.00133\n",
      "global step 1140, epoch: 1, batch: 1140, loss: 0.00146\n",
      "global step 1150, epoch: 1, batch: 1150, loss: 0.00130\n",
      "global step 1160, epoch: 1, batch: 1160, loss: 0.00984\n",
      "global step 1170, epoch: 1, batch: 1170, loss: 0.00143\n",
      "global step 1180, epoch: 1, batch: 1180, loss: 0.00191\n",
      "global step 1190, epoch: 1, batch: 1190, loss: 0.00178\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.00164\n",
      "global step 1210, epoch: 1, batch: 1210, loss: 0.00135\n",
      "global step 1220, epoch: 1, batch: 1220, loss: 0.00219\n",
      "global step 1230, epoch: 1, batch: 1230, loss: 0.00177\n",
      "global step 1240, epoch: 1, batch: 1240, loss: 0.00086\n",
      "global step 1250, epoch: 1, batch: 1250, loss: 0.00285\n",
      "global step 1260, epoch: 1, batch: 1260, loss: 0.00106\n",
      "global step 1270, epoch: 1, batch: 1270, loss: 0.00190\n",
      "global step 1280, epoch: 1, batch: 1280, loss: 0.00100\n",
      "global step 1290, epoch: 1, batch: 1290, loss: 0.00150\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.00240\n",
      "global step 1310, epoch: 1, batch: 1310, loss: 0.00129\n",
      "global step 1320, epoch: 1, batch: 1320, loss: 0.00180\n",
      "global step 1330, epoch: 1, batch: 1330, loss: 0.00057\n",
      "global step 1340, epoch: 1, batch: 1340, loss: 0.00254\n",
      "global step 1350, epoch: 1, batch: 1350, loss: 0.00266\n",
      "global step 1360, epoch: 1, batch: 1360, loss: 0.00067\n",
      "global step 1370, epoch: 1, batch: 1370, loss: 0.00121\n",
      "global step 1380, epoch: 1, batch: 1380, loss: 0.00416\n",
      "global step 1390, epoch: 1, batch: 1390, loss: 0.00318\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.00259\n",
      "global step 1410, epoch: 1, batch: 1410, loss: 0.00175\n",
      "global step 1420, epoch: 1, batch: 1420, loss: 0.00087\n",
      "global step 1430, epoch: 1, batch: 1430, loss: 0.00220\n",
      "global step 1440, epoch: 1, batch: 1440, loss: 0.00070\n",
      "global step 1450, epoch: 1, batch: 1450, loss: 0.00152\n",
      "global step 1460, epoch: 1, batch: 1460, loss: 0.00195\n",
      "global step 1470, epoch: 1, batch: 1470, loss: 0.00078\n",
      "global step 1480, epoch: 1, batch: 1480, loss: 0.00083\n",
      "global step 1490, epoch: 1, batch: 1490, loss: 0.00129\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.00130\n",
      "global step 1510, epoch: 1, batch: 1510, loss: 0.00089\n",
      "global step 1520, epoch: 1, batch: 1520, loss: 0.00103\n",
      "global step 1530, epoch: 1, batch: 1530, loss: 0.00113\n",
      "global step 1540, epoch: 1, batch: 1540, loss: 0.00062\n",
      "global step 1550, epoch: 1, batch: 1550, loss: 0.00131\n",
      "global step 1560, epoch: 1, batch: 1560, loss: 0.00081\n",
      "global step 1570, epoch: 1, batch: 1570, loss: 0.00230\n",
      "global step 1580, epoch: 1, batch: 1580, loss: 0.00116\n",
      "global step 1590, epoch: 1, batch: 1590, loss: 0.00087\n",
      "global step 1600, epoch: 1, batch: 1600, loss: 0.00108\n",
      "global step 1610, epoch: 1, batch: 1610, loss: 0.00101\n",
      "global step 1620, epoch: 1, batch: 1620, loss: 0.00177\n",
      "global step 1630, epoch: 1, batch: 1630, loss: 0.00066\n",
      "global step 1640, epoch: 1, batch: 1640, loss: 0.00088\n",
      "global step 1650, epoch: 1, batch: 1650, loss: 0.00212\n",
      "global step 1660, epoch: 1, batch: 1660, loss: 0.00117\n",
      "global step 1670, epoch: 1, batch: 1670, loss: 0.00170\n",
      "global step 1680, epoch: 1, batch: 1680, loss: 0.00064\n",
      "global step 1690, epoch: 1, batch: 1690, loss: 0.00145\n",
      "global step 1700, epoch: 1, batch: 1700, loss: 0.00102\n",
      "global step 1710, epoch: 1, batch: 1710, loss: 0.00221\n",
      "global step 1720, epoch: 1, batch: 1720, loss: 0.00088\n",
      "global step 1730, epoch: 1, batch: 1730, loss: 0.00118\n",
      "global step 1740, epoch: 1, batch: 1740, loss: 0.00143\n",
      "global step 1750, epoch: 1, batch: 1750, loss: 0.00046\n",
      "global step 1760, epoch: 1, batch: 1760, loss: 0.00112\n",
      "global step 1770, epoch: 1, batch: 1770, loss: 0.00214\n",
      "global step 1780, epoch: 1, batch: 1780, loss: 0.00150\n",
      "global step 1790, epoch: 1, batch: 1790, loss: 0.00090\n",
      "global step 1800, epoch: 1, batch: 1800, loss: 0.00087\n",
      "global step 1810, epoch: 1, batch: 1810, loss: 0.00196\n",
      "global step 1820, epoch: 1, batch: 1820, loss: 0.00071\n",
      "global step 1830, epoch: 1, batch: 1830, loss: 0.00139\n",
      "global step 1840, epoch: 1, batch: 1840, loss: 0.00305\n",
      "global step 1850, epoch: 1, batch: 1850, loss: 0.00159\n",
      "global step 1860, epoch: 1, batch: 1860, loss: 0.00600\n",
      "global step 1870, epoch: 1, batch: 1870, loss: 0.00102\n",
      "global step 1880, epoch: 1, batch: 1880, loss: 0.00083\n",
      "global step 1890, epoch: 1, batch: 1890, loss: 0.00770\n",
      "global step 1900, epoch: 1, batch: 1900, loss: 0.00213\n",
      "global step 1910, epoch: 1, batch: 1910, loss: 0.00114\n",
      "global step 1920, epoch: 1, batch: 1920, loss: 0.00103\n",
      "global step 1930, epoch: 1, batch: 1930, loss: 0.00131\n",
      "global step 1940, epoch: 1, batch: 1940, loss: 0.00080\n",
      "global step 1950, epoch: 1, batch: 1950, loss: 0.00096\n",
      "global step 1960, epoch: 1, batch: 1960, loss: 0.00089\n",
      "global step 1970, epoch: 1, batch: 1970, loss: 0.00041\n",
      "global step 1980, epoch: 1, batch: 1980, loss: 0.00068\n",
      "global step 1990, epoch: 1, batch: 1990, loss: 0.00142\n",
      "global step 2000, epoch: 1, batch: 2000, loss: 0.00062\n",
      "global step 2010, epoch: 1, batch: 2010, loss: 0.00108\n",
      "global step 2020, epoch: 1, batch: 2020, loss: 0.00088\n",
      "global step 2030, epoch: 1, batch: 2030, loss: 0.00073\n",
      "global step 2040, epoch: 1, batch: 2040, loss: 0.00085\n",
      "global step 2050, epoch: 1, batch: 2050, loss: 0.00140\n",
      "global step 2060, epoch: 1, batch: 2060, loss: 0.00115\n",
      "global step 2070, epoch: 1, batch: 2070, loss: 0.00287\n",
      "global step 2080, epoch: 1, batch: 2080, loss: 0.00072\n",
      "global step 2090, epoch: 1, batch: 2090, loss: 0.00067\n",
      "global step 2100, epoch: 1, batch: 2100, loss: 0.00042\n",
      "global step 2110, epoch: 1, batch: 2110, loss: 0.00087\n",
      "global step 2120, epoch: 1, batch: 2120, loss: 0.00081\n",
      "global step 2130, epoch: 1, batch: 2130, loss: 0.00185\n",
      "global step 2140, epoch: 1, batch: 2140, loss: 0.00050\n",
      "global step 2150, epoch: 1, batch: 2150, loss: 0.00132\n",
      "global step 2160, epoch: 1, batch: 2160, loss: 0.00090\n",
      "global step 2170, epoch: 1, batch: 2170, loss: 0.00052\n",
      "global step 2180, epoch: 1, batch: 2180, loss: 0.00081\n",
      "global step 2190, epoch: 1, batch: 2190, loss: 0.00067\n",
      "global step 2200, epoch: 1, batch: 2200, loss: 0.00120\n",
      "global step 2210, epoch: 1, batch: 2210, loss: 0.00103\n",
      "global step 2220, epoch: 1, batch: 2220, loss: 0.00079\n",
      "global step 2230, epoch: 1, batch: 2230, loss: 0.00089\n",
      "global step 2240, epoch: 1, batch: 2240, loss: 0.00071\n",
      "global step 2250, epoch: 1, batch: 2250, loss: 0.00070\n",
      "global step 2260, epoch: 1, batch: 2260, loss: 0.00102\n",
      "global step 2270, epoch: 1, batch: 2270, loss: 0.00051\n",
      "global step 2280, epoch: 1, batch: 2280, loss: 0.00075\n",
      "global step 2290, epoch: 1, batch: 2290, loss: 0.00120\n",
      "global step 2300, epoch: 1, batch: 2300, loss: 0.00069\n",
      "global step 2310, epoch: 1, batch: 2310, loss: 0.00143\n",
      "global step 2320, epoch: 1, batch: 2320, loss: 0.00077\n",
      "global step 2330, epoch: 1, batch: 2330, loss: 0.00153\n",
      "global step 2340, epoch: 1, batch: 2340, loss: 0.00087\n",
      "global step 2350, epoch: 1, batch: 2350, loss: 0.00090\n",
      "global step 2360, epoch: 1, batch: 2360, loss: 0.00065\n",
      "global step 2370, epoch: 1, batch: 2370, loss: 0.00130\n",
      "global step 2380, epoch: 1, batch: 2380, loss: 0.02682\n",
      "global step 2390, epoch: 1, batch: 2390, loss: 0.00284\n",
      "global step 2400, epoch: 1, batch: 2400, loss: 0.00240\n",
      "global step 2410, epoch: 1, batch: 2410, loss: 0.00055\n",
      "global step 2420, epoch: 1, batch: 2420, loss: 0.00130\n",
      "global step 2430, epoch: 1, batch: 2430, loss: 0.00105\n",
      "global step 2440, epoch: 1, batch: 2440, loss: 0.00061\n",
      "global step 2450, epoch: 1, batch: 2450, loss: 0.00126\n",
      "global step 2460, epoch: 1, batch: 2460, loss: 0.00148\n",
      "global step 2470, epoch: 1, batch: 2470, loss: 0.00059\n",
      "global step 2480, epoch: 1, batch: 2480, loss: 0.00073\n",
      "global step 2490, epoch: 1, batch: 2490, loss: 0.00082\n",
      "global step 2500, epoch: 1, batch: 2500, loss: 0.00081\n",
      "global step 2510, epoch: 1, batch: 2510, loss: 0.00149\n",
      "global step 2520, epoch: 1, batch: 2520, loss: 0.00063\n",
      "global step 2530, epoch: 1, batch: 2530, loss: 0.00092\n",
      "global step 2540, epoch: 1, batch: 2540, loss: 0.00275\n",
      "global step 2550, epoch: 1, batch: 2550, loss: 0.00091\n",
      "global step 2560, epoch: 1, batch: 2560, loss: 0.00081\n",
      "global step 2570, epoch: 1, batch: 2570, loss: 0.00116\n",
      "global step 2580, epoch: 1, batch: 2580, loss: 0.00067\n",
      "global step 2590, epoch: 1, batch: 2590, loss: 0.00072\n",
      "global step 2600, epoch: 1, batch: 2600, loss: 0.00056\n",
      "global step 2610, epoch: 1, batch: 2610, loss: 0.00120\n",
      "global step 2620, epoch: 1, batch: 2620, loss: 0.00122\n",
      "global step 2630, epoch: 1, batch: 2630, loss: 0.00057\n",
      "global step 2640, epoch: 1, batch: 2640, loss: 0.00111\n",
      "global step 2650, epoch: 1, batch: 2650, loss: 0.00066\n",
      "global step 2660, epoch: 1, batch: 2660, loss: 0.00043\n",
      "global step 2670, epoch: 1, batch: 2670, loss: 0.00051\n",
      "global step 2680, epoch: 1, batch: 2680, loss: 0.00064\n",
      "global step 2690, epoch: 1, batch: 2690, loss: 0.05202\n",
      "global step 2700, epoch: 1, batch: 2700, loss: 0.01724\n",
      "global step 2710, epoch: 1, batch: 2710, loss: 0.01772\n",
      "global step 2720, epoch: 1, batch: 2720, loss: 0.04199\n",
      "global step 2730, epoch: 1, batch: 2730, loss: 0.02551\n",
      "global step 2740, epoch: 1, batch: 2740, loss: 0.01174\n",
      "global step 2750, epoch: 1, batch: 2750, loss: 0.01773\n",
      "global step 2760, epoch: 1, batch: 2760, loss: 0.00680\n",
      "global step 2770, epoch: 1, batch: 2770, loss: 0.01601\n",
      "global step 2780, epoch: 1, batch: 2780, loss: 0.00593\n",
      "global step 2790, epoch: 1, batch: 2790, loss: 0.01300\n",
      "global step 2800, epoch: 1, batch: 2800, loss: 0.00312\n",
      "global step 2810, epoch: 1, batch: 2810, loss: 0.00639\n",
      "global step 2820, epoch: 1, batch: 2820, loss: 0.00274\n",
      "global step 2830, epoch: 1, batch: 2830, loss: 0.00514\n",
      "global step 2840, epoch: 1, batch: 2840, loss: 0.00258\n",
      "global step 2850, epoch: 1, batch: 2850, loss: 0.00361\n",
      "global step 2860, epoch: 1, batch: 2860, loss: 0.00184\n",
      "global step 2870, epoch: 1, batch: 2870, loss: 0.00201\n",
      "global step 2880, epoch: 1, batch: 2880, loss: 0.00199\n",
      "global step 2890, epoch: 1, batch: 2890, loss: 0.00136\n",
      "global step 2900, epoch: 1, batch: 2900, loss: 0.00219\n",
      "global step 2910, epoch: 1, batch: 2910, loss: 0.00521\n",
      "global step 2920, epoch: 1, batch: 2920, loss: 0.00301\n",
      "global step 2930, epoch: 1, batch: 2930, loss: 0.00099\n",
      "global step 2940, epoch: 1, batch: 2940, loss: 0.00318\n",
      "global step 2950, epoch: 1, batch: 2950, loss: 0.00151\n",
      "global step 2960, epoch: 1, batch: 2960, loss: 0.00606\n",
      "global step 2970, epoch: 1, batch: 2970, loss: 0.20863\n",
      "global step 2980, epoch: 1, batch: 2980, loss: 0.12528\n",
      "global step 2990, epoch: 1, batch: 2990, loss: 0.08279\n",
      "global step 3000, epoch: 1, batch: 3000, loss: 0.32276\n",
      "global step 3010, epoch: 1, batch: 3010, loss: 0.06407\n",
      "global step 3020, epoch: 1, batch: 3020, loss: 0.04611\n",
      "global step 3030, epoch: 1, batch: 3030, loss: 0.29176\n",
      "global step 3040, epoch: 1, batch: 3040, loss: 0.15449\n",
      "global step 3050, epoch: 1, batch: 3050, loss: 0.29071\n",
      "global step 3060, epoch: 1, batch: 3060, loss: 0.13064\n",
      "global step 3070, epoch: 1, batch: 3070, loss: 0.06039\n",
      "global step 3080, epoch: 1, batch: 3080, loss: 0.02966\n",
      "global step 3090, epoch: 1, batch: 3090, loss: 0.45383\n",
      "global step 3100, epoch: 1, batch: 3100, loss: 0.36351\n",
      "global step 3110, epoch: 1, batch: 3110, loss: 0.18282\n",
      "global step 3120, epoch: 1, batch: 3120, loss: 0.06148\n",
      "global step 3130, epoch: 1, batch: 3130, loss: 0.23530\n",
      "global step 3140, epoch: 1, batch: 3140, loss: 0.02299\n",
      "global step 3150, epoch: 1, batch: 3150, loss: 0.03044\n",
      "global step 3160, epoch: 1, batch: 3160, loss: 0.05983\n",
      "global step 3170, epoch: 1, batch: 3170, loss: 0.07319\n",
      "global step 3180, epoch: 1, batch: 3180, loss: 0.04465\n",
      "global step 3190, epoch: 1, batch: 3190, loss: 0.05041\n",
      "global step 3200, epoch: 1, batch: 3200, loss: 0.04416\n",
      "global step 3210, epoch: 1, batch: 3210, loss: 0.03032\n",
      "global step 3220, epoch: 1, batch: 3220, loss: 0.02284\n",
      "global step 3230, epoch: 1, batch: 3230, loss: 0.01772\n",
      "global step 3240, epoch: 1, batch: 3240, loss: 0.00538\n",
      "global step 3250, epoch: 1, batch: 3250, loss: 0.00966\n",
      "global step 3260, epoch: 1, batch: 3260, loss: 0.00378\n",
      "global step 3270, epoch: 1, batch: 3270, loss: 0.02049\n",
      "global step 3280, epoch: 1, batch: 3280, loss: 0.00568\n",
      "global step 3290, epoch: 1, batch: 3290, loss: 0.00752\n",
      "global step 3300, epoch: 1, batch: 3300, loss: 0.01912\n",
      "global step 3310, epoch: 1, batch: 3310, loss: 0.01228\n",
      "global step 3320, epoch: 1, batch: 3320, loss: 0.00690\n",
      "global step 3330, epoch: 1, batch: 3330, loss: 0.00266\n",
      "global step 3340, epoch: 1, batch: 3340, loss: 0.01056\n",
      "global step 3350, epoch: 1, batch: 3350, loss: 0.00398\n",
      "global step 3360, epoch: 1, batch: 3360, loss: 0.01485\n",
      "global step 3370, epoch: 1, batch: 3370, loss: 0.00510\n",
      "global step 3380, epoch: 1, batch: 3380, loss: 0.00868\n",
      "global step 3390, epoch: 1, batch: 3390, loss: 0.00375\n",
      "global step 3400, epoch: 1, batch: 3400, loss: 0.01304\n",
      "global step 3410, epoch: 1, batch: 3410, loss: 0.00402\n",
      "global step 3420, epoch: 1, batch: 3420, loss: 0.01389\n",
      "global step 3430, epoch: 1, batch: 3430, loss: 0.00327\n",
      "global step 3440, epoch: 1, batch: 3440, loss: 0.00356\n",
      "global step 3450, epoch: 1, batch: 3450, loss: 0.00602\n",
      "global step 3460, epoch: 1, batch: 3460, loss: 0.00306\n",
      "global step 3470, epoch: 1, batch: 3470, loss: 0.01166\n",
      "global step 3480, epoch: 1, batch: 3480, loss: 0.00520\n",
      "global step 3490, epoch: 1, batch: 3490, loss: 0.00396\n",
      "global step 3500, epoch: 1, batch: 3500, loss: 0.00997\n",
      "global step 3510, epoch: 1, batch: 3510, loss: 0.00206\n",
      "global step 3520, epoch: 1, batch: 3520, loss: 0.00316\n",
      "global step 3530, epoch: 1, batch: 3530, loss: 0.00171\n",
      "global step 3540, epoch: 1, batch: 3540, loss: 0.00677\n",
      "global step 3550, epoch: 1, batch: 3550, loss: 0.00297\n",
      "global step 3560, epoch: 1, batch: 3560, loss: 0.00222\n",
      "global step 3570, epoch: 1, batch: 3570, loss: 0.00133\n",
      "global step 3580, epoch: 1, batch: 3580, loss: 0.00170\n",
      "global step 3590, epoch: 1, batch: 3590, loss: 0.00171\n",
      "global step 3600, epoch: 1, batch: 3600, loss: 0.00331\n",
      "global step 3610, epoch: 1, batch: 3610, loss: 0.00252\n",
      "global step 3620, epoch: 1, batch: 3620, loss: 0.00269\n",
      "global step 3630, epoch: 1, batch: 3630, loss: 0.00301\n",
      "global step 3640, epoch: 1, batch: 3640, loss: 0.04059\n",
      "global step 3650, epoch: 1, batch: 3650, loss: 0.00327\n",
      "global step 3660, epoch: 1, batch: 3660, loss: 0.00423\n",
      "global step 3670, epoch: 1, batch: 3670, loss: 0.00546\n",
      "global step 3680, epoch: 1, batch: 3680, loss: 0.02193\n",
      "global step 3690, epoch: 1, batch: 3690, loss: 0.00182\n",
      "global step 3700, epoch: 1, batch: 3700, loss: 0.00383\n",
      "global step 3710, epoch: 1, batch: 3710, loss: 0.00369\n",
      "global step 3720, epoch: 1, batch: 3720, loss: 0.00280\n",
      "global step 3730, epoch: 1, batch: 3730, loss: 0.00187\n",
      "global step 3740, epoch: 1, batch: 3740, loss: 0.00309\n",
      "global step 3750, epoch: 1, batch: 3750, loss: 0.00202\n",
      "global step 3760, epoch: 1, batch: 3760, loss: 0.00119\n",
      "global step 3770, epoch: 1, batch: 3770, loss: 0.00151\n",
      "global step 3780, epoch: 1, batch: 3780, loss: 0.00826\n",
      "global step 3790, epoch: 1, batch: 3790, loss: 0.00238\n",
      "global step 3800, epoch: 1, batch: 3800, loss: 0.01512\n",
      "global step 3810, epoch: 1, batch: 3810, loss: 0.00229\n",
      "global step 3820, epoch: 1, batch: 3820, loss: 0.00454\n",
      "global step 3830, epoch: 1, batch: 3830, loss: 0.02650\n",
      "global step 3840, epoch: 1, batch: 3840, loss: 0.00233\n",
      "global step 3850, epoch: 1, batch: 3850, loss: 0.00258\n",
      "global step 3860, epoch: 1, batch: 3860, loss: 0.00415\n",
      "global step 3870, epoch: 1, batch: 3870, loss: 0.00438\n",
      "global step 3880, epoch: 1, batch: 3880, loss: 0.00292\n",
      "global step 3890, epoch: 1, batch: 3890, loss: 0.00202\n",
      "global step 3900, epoch: 1, batch: 3900, loss: 0.00331\n",
      "global step 3910, epoch: 1, batch: 3910, loss: 0.01865\n",
      "global step 3920, epoch: 1, batch: 3920, loss: 0.00161\n",
      "global step 3930, epoch: 1, batch: 3930, loss: 0.00651\n",
      "global step 3940, epoch: 1, batch: 3940, loss: 0.00148\n",
      "global step 3950, epoch: 1, batch: 3950, loss: 0.00233\n",
      "global step 3960, epoch: 1, batch: 3960, loss: 0.00303\n",
      "global step 3970, epoch: 1, batch: 3970, loss: 0.00943\n",
      "global step 3980, epoch: 1, batch: 3980, loss: 0.00351\n",
      "global step 3990, epoch: 1, batch: 3990, loss: 0.00417\n",
      "global step 4000, epoch: 1, batch: 4000, loss: 0.00599\n",
      "global step 4010, epoch: 1, batch: 4010, loss: 0.00405\n",
      "global step 4020, epoch: 1, batch: 4020, loss: 0.00374\n",
      "global step 4030, epoch: 1, batch: 4030, loss: 0.00466\n",
      "global step 4040, epoch: 1, batch: 4040, loss: 0.00295\n",
      "global step 4050, epoch: 1, batch: 4050, loss: 0.00170\n",
      "global step 4060, epoch: 1, batch: 4060, loss: 0.00448\n",
      "global step 4070, epoch: 1, batch: 4070, loss: 0.00257\n",
      "global step 4080, epoch: 1, batch: 4080, loss: 0.00138\n",
      "global step 4090, epoch: 1, batch: 4090, loss: 0.00121\n",
      "global step 4100, epoch: 1, batch: 4100, loss: 0.00262\n",
      "global step 4110, epoch: 1, batch: 4110, loss: 0.00696\n",
      "global step 4120, epoch: 1, batch: 4120, loss: 0.00303\n",
      "global step 4130, epoch: 1, batch: 4130, loss: 0.00998\n",
      "global step 4140, epoch: 1, batch: 4140, loss: 0.00307\n",
      "global step 4150, epoch: 1, batch: 4150, loss: 0.00190\n",
      "global step 4160, epoch: 1, batch: 4160, loss: 0.00679\n",
      "global step 4170, epoch: 1, batch: 4170, loss: 0.00133\n",
      "global step 4180, epoch: 1, batch: 4180, loss: 0.00106\n",
      "global step 4190, epoch: 1, batch: 4190, loss: 0.00152\n",
      "global step 4200, epoch: 1, batch: 4200, loss: 0.00167\n",
      "global step 4210, epoch: 1, batch: 4210, loss: 0.00203\n",
      "global step 4220, epoch: 1, batch: 4220, loss: 0.00132\n",
      "global step 4230, epoch: 1, batch: 4230, loss: 0.00302\n",
      "global step 4240, epoch: 1, batch: 4240, loss: 0.00107\n",
      "global step 4250, epoch: 1, batch: 4250, loss: 0.00406\n",
      "global step 4260, epoch: 1, batch: 4260, loss: 0.00450\n",
      "global step 4270, epoch: 1, batch: 4270, loss: 0.00198\n",
      "global step 4280, epoch: 1, batch: 4280, loss: 0.00169\n",
      "global step 4290, epoch: 1, batch: 4290, loss: 0.00171\n",
      "global step 4300, epoch: 1, batch: 4300, loss: 0.00199\n",
      "global step 4310, epoch: 1, batch: 4310, loss: 0.00176\n",
      "global step 4320, epoch: 1, batch: 4320, loss: 0.00219\n",
      "global step 4330, epoch: 1, batch: 4330, loss: 0.00102\n",
      "global step 4340, epoch: 1, batch: 4340, loss: 0.00113\n",
      "global step 4350, epoch: 1, batch: 4350, loss: 0.00122\n",
      "global step 4360, epoch: 1, batch: 4360, loss: 0.00333\n",
      "global step 4370, epoch: 1, batch: 4370, loss: 0.00301\n",
      "global step 4380, epoch: 1, batch: 4380, loss: 0.00152\n",
      "global step 4390, epoch: 1, batch: 4390, loss: 0.00522\n",
      "global step 4400, epoch: 1, batch: 4400, loss: 0.01384\n",
      "global step 4410, epoch: 1, batch: 4410, loss: 0.00274\n",
      "global step 4420, epoch: 1, batch: 4420, loss: 0.00125\n",
      "global step 4430, epoch: 1, batch: 4430, loss: 0.01086\n",
      "global step 4440, epoch: 1, batch: 4440, loss: 0.00127\n",
      "global step 4450, epoch: 1, batch: 4450, loss: 0.00158\n",
      "global step 4460, epoch: 1, batch: 4460, loss: 0.00271\n",
      "global step 4470, epoch: 1, batch: 4470, loss: 0.00269\n",
      "global step 4480, epoch: 1, batch: 4480, loss: 0.00154\n",
      "global step 4490, epoch: 1, batch: 4490, loss: 0.00215\n",
      "global step 4500, epoch: 1, batch: 4500, loss: 0.00121\n",
      "global step 4510, epoch: 1, batch: 4510, loss: 0.00359\n",
      "global step 4520, epoch: 1, batch: 4520, loss: 0.00163\n",
      "global step 4530, epoch: 1, batch: 4530, loss: 0.00100\n",
      "global step 4540, epoch: 1, batch: 4540, loss: 0.00118\n",
      "global step 4550, epoch: 1, batch: 4550, loss: 0.00094\n",
      "global step 4560, epoch: 1, batch: 4560, loss: 0.00151\n",
      "global step 4570, epoch: 1, batch: 4570, loss: 0.00449\n",
      "global step 4580, epoch: 1, batch: 4580, loss: 0.00237\n",
      "global step 4590, epoch: 1, batch: 4590, loss: 0.00236\n",
      "global step 4600, epoch: 1, batch: 4600, loss: 0.01961\n",
      "global step 4610, epoch: 1, batch: 4610, loss: 0.00231\n",
      "global step 4620, epoch: 1, batch: 4620, loss: 0.00202\n",
      "global step 4630, epoch: 1, batch: 4630, loss: 0.00162\n",
      "global step 4640, epoch: 1, batch: 4640, loss: 0.00228\n",
      "global step 4650, epoch: 1, batch: 4650, loss: 0.00365\n",
      "global step 4660, epoch: 1, batch: 4660, loss: 0.00703\n",
      "global step 4670, epoch: 1, batch: 4670, loss: 0.00133\n",
      "global step 4680, epoch: 1, batch: 4680, loss: 0.00241\n",
      "global step 4690, epoch: 1, batch: 4690, loss: 0.00190\n",
      "global step 4700, epoch: 1, batch: 4700, loss: 0.00164\n",
      "global step 4710, epoch: 1, batch: 4710, loss: 0.00137\n",
      "global step 4720, epoch: 1, batch: 4720, loss: 0.00221\n",
      "global step 4730, epoch: 1, batch: 4730, loss: 0.00251\n",
      "global step 4740, epoch: 1, batch: 4740, loss: 0.00190\n",
      "global step 4750, epoch: 1, batch: 4750, loss: 0.00143\n",
      "global step 4760, epoch: 1, batch: 4760, loss: 0.00080\n",
      "global step 4770, epoch: 1, batch: 4770, loss: 0.00109\n",
      "global step 4780, epoch: 1, batch: 4780, loss: 0.00181\n",
      "global step 4790, epoch: 1, batch: 4790, loss: 0.00233\n",
      "global step 4800, epoch: 1, batch: 4800, loss: 0.00141\n",
      "global step 4810, epoch: 1, batch: 4810, loss: 0.00128\n",
      "global step 4820, epoch: 1, batch: 4820, loss: 0.00358\n",
      "global step 4830, epoch: 1, batch: 4830, loss: 0.00075\n",
      "global step 4840, epoch: 1, batch: 4840, loss: 0.00169\n",
      "global step 4850, epoch: 1, batch: 4850, loss: 0.00307\n",
      "global step 4860, epoch: 1, batch: 4860, loss: 0.00100\n",
      "global step 4870, epoch: 1, batch: 4870, loss: 0.00094\n",
      "global step 4880, epoch: 1, batch: 4880, loss: 0.00066\n",
      "global step 4890, epoch: 1, batch: 4890, loss: 0.00351\n",
      "global step 4900, epoch: 1, batch: 4900, loss: 0.00053\n",
      "global step 4910, epoch: 1, batch: 4910, loss: 0.00628\n",
      "global step 4920, epoch: 1, batch: 4920, loss: 0.00133\n",
      "global step 4930, epoch: 1, batch: 4930, loss: 0.00148\n",
      "global step 4940, epoch: 1, batch: 4940, loss: 0.00215\n",
      "global step 4950, epoch: 1, batch: 4950, loss: 0.00086\n",
      "global step 4960, epoch: 1, batch: 4960, loss: 0.00111\n",
      "global step 4970, epoch: 1, batch: 4970, loss: 0.00121\n",
      "global step 4980, epoch: 1, batch: 4980, loss: 0.00195\n",
      "global step 4990, epoch: 1, batch: 4990, loss: 0.00103\n",
      "global step 5000, epoch: 1, batch: 5000, loss: 0.00083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-08-06 01:12:07,830] [    INFO] - tokenizer config file saved in checkpoint/tokenizer_config.json\n",
      "[2022-08-06 01:12:07,834] [    INFO] - Special tokens file saved in checkpoint/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"checkpoint\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        query_input_ids, title_input_ids = batch\n",
    "        # 其中query和title为同一条数据\n",
    "        loss = model(\n",
    "                query_input_ids=query_input_ids,\n",
    "                title_input_ids=title_input_ids)\n",
    "        # 每隔10个step进行打印日志\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f\" % (global_step, epoch, step, loss))\n",
    "        # 反向\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if step == 5000:\n",
    "            break\n",
    "    \n",
    "    # 每一轮都保存模型\n",
    "    save_param_path = os.path.join(save_dir, 'model_state.pdparams')  # 保存模型参数\n",
    "    save_lr_path = os.path.join(save_dir, 'lr')\n",
    "    save_opt_path = os.path.join(save_dir, 'opt')\n",
    "\n",
    "    paddle.save(model.state_dict(), save_param_path)\n",
    "    paddle.save(lr_scheduler.state_dict(), save_lr_path)\n",
    "    paddle.save(optimizer.state_dict(), save_opt_path)\n",
    "    tokenizer.save_pretrained(save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
